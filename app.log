INFO:root:main.py initialise
INFO:root:main() starting
INFO:root:being checks
INFO:root:Checks passed
DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions
DEBUG:openai:api_version=None data='{"model": "gpt-4-0613", "messages": [{"role": "system", "content": "I will provide you with piece of content (e.g. articles, papers, documentation, etc.)\\n\\n        You will generate increasingly concise, entity-dense summaries of the content.\\n\\n        Repeat the following 2 steps 5 times.\\n\\n        Step 1. Identify 1-3 informative Entities (\\";\\" delimited) from the Article which are missing from the previously generated summary.\\n\\n        Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities.\\n\\n        A Missing Entity is:\\n\\n        Relevant: to the main story.\\n        Specific: descriptive yet concise (5 words or fewer).\\n        Novel: not in the previous summary.\\n        Faithful: present in the content piece.\\n        Anywhere: located anywhere in the Article.\\n\\n        Guidelines:\\n\\n        The first summary should be long (4 - 5 sentences, -80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \\"this article discusses\\") to reach -80 words.\\n        Make every word count: re-write the previous summary to improve flow and make space for additional entities.\\n        Make space with fusion, compression, and removal of uninformative phrases like \\"the article discusses\\".\\n        The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.\\n        Missing entities can appear anywhere in the new summary.\\n        Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\\n        Remember, use the exact same number of words for each summary.\\n        Answer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are \\"Missing_Entities\\" and \\"Denser_Summary\\"."}, {"role": "user", "content": "Here is the input text for you to summarise using the \'Missing_Entities\' and \'Denser_Summary\' approach:\\n\\nWe present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shift short attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-efficient fine-tuning regime for context expansion. Notably, we find that LoRA for context extension works well under the premise of trainable embedding and normalization. LongLoRA demonstrates strong empirical results on various tasks on LLaMA2 models from 7B/13B to 70B. LongLoRA adopts LLaMA2 7B from 4k context to 100k, or LLaMA2 70B to 32k on a single 8x A100 machine. LongLoRA extends models\' context while retaining their original architectures, and is compatible with most existing techniques, like FlashAttention-2. In addition, to make LongLoRA practical, we collect a dataset, LongQA, for supervised fine-tuning. It contains more than 3k long context question-answer pairs."}], "temperature": 1.0, "top_p": 1.0, "n": 1, "stream": false, "stop": null, "max_tokens": null, "presence_penalty": 0, "frequency_penalty": 0}' message='Post details'
DEBUG:urllib3.util.retry:Converted retries value: 2 -> Retry(total=2, connect=None, read=None, redirect=None, status=None)
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.openai.com:443
DEBUG:urllib3.connectionpool:https://api.openai.com:443 "POST /v1/chat/completions HTTP/1.1" 200 None
DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=42902 request_id=92d95008e7e563bcfb6735a8fe3b80b8 response_code=200
INFO:root:[
{
"Missing_Entities": "LongLoRA; models' context extension; computation cost reduction",
"Denser_Summary": "This article discusses LongLoRA, an efficient fine-tuning approach designed specifically to extend the context sizes of pre-trained large language models (LLMs). The technique focuses on overcoming the significant computation cost reduction associated with the process. Conventionally, the effort to train LLMs with extended context sizes is a highly demanding task, requiring significant resource inputs including extensive training hours and GPU resources. LongLoRA seeks to address these challenges by looking to enhance the speed of models' context extension."
},

{
"Missing_Entities": "Sparse local attention; parameter-efficient fine-tuning regime; trainable embedding and normalization",
"Denser_Summary": "LongLoRA tackles context size extension in pre-trained LLMs, offering a solution to reduced computation costs. Besides, it next employs sparse local attention instead of dense global attention to effectually enhance fine-tuning. Furthermore, it features a parameter-efficient fine-tuning regime that is effective for context size expansion, chiefly achieved upon the premise of trainable embedding and normalization."
},

{
"Missing_Entities": "LLaMA2 models; 8x A100 machine; FlashAttention-2 compatibility",
"Denser_Summary": "LongLoRA, focusing on computation cost reduction, leverages techniques like sparse local attention and the parameter-efficient fine-tuning regime. It exhibits remarkable results when applied to LLaMA2 models of various scales, from 7B/13B to 70B, achieved on a singular 8x A100 machine. More so, it maintains model architectures' originality while extending their context and exhibits undeterred compatibility with current methodologies like FlashAttention-2."
},

{
"Missing_Entities": "LongQA dataset; 3k long context QA pairs; supervised fine-tuning",
"Denser_Summary": "LongLoRA optimizes LLaMA2 models' context extension while minimizing computational expense using sparse local attention and a parameter-efficient regimen. It's seamlessly compatible with FlashAttention-2 while retaining the original model architecture. To aid practicality, the research team has compiled the LongQA dataset, consisting of over 3k long-context question-answer pairs, introducing the concept of supervised fine-tuning."
},

{
"Missing_Entities": "Code implementation simplicity; original architecture retention; technique's practicality",
"Denser_Summary": "LongLoRA enhances LLaMA2 models' context extension with computational economy, local attention sparsity, and a configured fine-tuning regime. The method, compatible with FlashAttention-2, retains models' architecture, made accessible through minimal lines of coding. To underscore its practicality, researchers collected the LongQA dataset with 3k+ long-context QA pairs, facilitating supervised fine-tuning."
}
]
INFO:root:main.py initialise
INFO:root:main() starting
INFO:root:being checks
INFO:root:Checks passed
DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions
DEBUG:openai:api_version=None data='{"model": "gpt-4-0613", "messages": [{"role": "system", "content": "I will provide you with piece of content (e.g. articles, papers, documentation, etc.)\\n\\n        You will generate increasingly concise, entity-dense summaries of the content.\\n\\n        Repeat the following 2 steps 5 times.\\n\\n        Step 1. Identify 1-3 informative Entities (\\";\\" delimited) from the Article which are missing from the previously generated summary.\\n\\n        Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities.\\n\\n        A Missing Entity is:\\n\\n        Relevant: to the main story.\\n        Specific: descriptive yet concise (5 words or fewer).\\n        Novel: not in the previous summary.\\n        Faithful: present in the content piece.\\n        Anywhere: located anywhere in the Article.\\n\\n        Guidelines:\\n\\n        The first summary should be long (4 - 5 sentences, -80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \\"this article discusses\\") to reach -80 words.\\n        Make every word count: re-write the previous summary to improve flow and make space for additional entities.\\n        Make space with fusion, compression, and removal of uninformative phrases like \\"the article discusses\\".\\n        The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.\\n        Missing entities can appear anywhere in the new summary.\\n        Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\\n        Remember, use the exact same number of words for each summary.\\n        Answer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are \\"Missing_Entities\\" and \\"Denser_Summary\\"."}, {"role": "user", "content": "Here is the input text for you to summarise using the \'Missing_Entities\' and \'Denser_Summary\' approach:\\n\\nWe present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shift short attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-efficient fine-tuning regime for context expansion. Notably, we find that LoRA for context extension works well under the premise of trainable embedding and normalization. LongLoRA demonstrates strong empirical results on various tasks on LLaMA2 models from 7B/13B to 70B. LongLoRA adopts LLaMA2 7B from 4k context to 100k, or LLaMA2 70B to 32k on a single 8x A100 machine. LongLoRA extends models\' context while retaining their original architectures, and is compatible with most existing techniques, like FlashAttention-2. In addition, to make LongLoRA practical, we collect a dataset, LongQA, for supervised fine-tuning. It contains more than 3k long context question-answer pairs."}], "temperature": 1.0, "top_p": 1.0, "n": 1, "stream": false, "stop": null, "max_tokens": null, "presence_penalty": 0, "frequency_penalty": 0}' message='Post details'
DEBUG:urllib3.util.retry:Converted retries value: 2 -> Retry(total=2, connect=None, read=None, redirect=None, status=None)
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.openai.com:443
DEBUG:urllib3.connectionpool:https://api.openai.com:443 "POST /v1/chat/completions HTTP/1.1" 200 None
DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=31254 request_id=259f078b5282c3f99c349b4733a7d7a6 response_code=200
INFO:root:[
{
"Missing_Entities": "LongLoRA; pre-trained large language models (LLMs); context sizes",
"Denser_Summary": "The article discusses a new method known as LongLoRA, which is shown to be an efficient means of fine-tuning pre-trained large language models (LLMs). The main advantage of this method lies in its extension of context sizes at a reduced computational cost. Whereas training LLMs with extended context sizes usually comes with a burdensome computational demand, LongLoRA shows promise for its economical use of computational resources while still extending context sizes."
},
{
"Missing_Entities": "dense global attention; sparse local attention; shift short attention",
"Denser_Summary": "LongLoRA extends context sizes in LLMs using efficient fine-tuning and two lines of code. It leverages sparse local attention rather than the traditionally used dense global attention during inference. The method employs 'shift short attention', a novel implementation that ensures less computation while maintaining performance levels similar to those achieved with standard attention mechanisms."
},
{
"Missing_Entities": "parameter-efficient fine-tuning regime; LoRA for context extension; trainable embedding and normalization",
"Denser_Summary": "LongLoRA fine-tunes models using a parameter-efficient regime and the concept of LoRA, which expands context under the condition of trainable embedding and normalization. This fine-tuning approach, within its premise of constrained resources, excellently manages to extend the context of pre-trained LLMs via sparse local attendance, and shift short attention."
},
{
"Missing_Entities": "LLaMA2 models; FlashAttention-2; LongQA dataset",
"Denser_Summary": "LongLoRA delivers potent results across tasks on different LLaMA2 models, extending the context of LLaMA2 7B or 70B models. This approach retains the original LLM architectures and collaborates seamlessly with existing strategies like FlashAttention-2. Moreover, to practically implement LongLoRA, a LongQA dataset has been compiled, supporting supervised fine-tuning."
},
{
"Missing_Entities": "Compatible with most techniques; LongQA's 3k question-answer pairs; context retained architecture",
"Denser_Summary": "LongLoRA, an efficient method to extend LLaMA2 model context sizes, aligns well with techniques like FlashAttention-2, preserving original architectures. Using sparse local attention, shift short attention, and parameter-efficient regimes, it fine-tunes models with the help of a compiled LongQA dataset that holds over 3k long context question-answer pairs for supervision."
}
]
